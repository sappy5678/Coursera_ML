{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./kc_house_train_data.csv\")\n",
    "test_data = pd.read_csv(\"./kc_house_test_data.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 給定權重時的預測輸出\n",
    "Suppose we had the weights [1.0, 1.0] and the features [1.0, 1180.0] and we wanted to compute the predicted output 1.0*1.0 + 1.0*1180.0 = 1181.0 this is the dot product between these two arrays. If they're numpy arrayws we can use np.dot() to compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
    "    # create the predictions vector by using np.dot()\n",
    "#     predictions = feature_matrix.dot(weights)\n",
    "    predictions = np.dot(feature_matrix,weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17384, 2)\n",
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "train_data[\"constant\"] = 1.\n",
    "test_data[\"constant\"] = 1.\n",
    "example_features = train_data[[\"constant\",\"sqft_living\"]]\n",
    "example_output = train_data[\"price\"]\n",
    "# print (example_features)\n",
    "my_weights = pd.Series([1,1])\n",
    "print (example_features.shape)\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print (test_predictions[0]) # should be 1181.0\n",
    "print (test_predictions[1]) # should be 2571.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Derivative\n",
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
    "\n",
    "(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k] - output)^2\n",
    "\n",
    "Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:\n",
    "\n",
    "2\\*(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k] - output)\\* [feature_i]\n",
    "\n",
    "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
    "\n",
    "2\\*error\\*[feature_i]\n",
    "\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors. \n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\n",
    "\n",
    "like this\n",
    "![公式圖](http://i.imgur.com/2VwUikP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    derivative =  2 * (np.dot(errors , feature))\n",
    "    derivative = pd.Series(derivative)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derivative 0   -1.875270e+10\n",
      "dtype: float64\n",
      "-np.sum(example_output)*2 -18752698920.0\n"
     ]
    }
   ],
   "source": [
    "test[\"constant\"] = 1.\n",
    "test_features = test[[\"constant\",\"sqft_living\"]]\n",
    "test_output = test[\"price\"]\n",
    "\n",
    "my_weights = pd.Series([0, 0]) # this makes all the predictions 0\n",
    "example_predictions = predict_output(example_features, my_weights) \n",
    "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
    "errors = example_predictions - example_output # prediction errors in this case is just the -example_output\n",
    "feature = example_features.values[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print (\"derivative\" ,derivative)\n",
    "print (\"-np.sum(example_output)*2\",-np.sum(example_output)*2) # should be the same as derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "![虛擬碼](http://i.imgur.com/ix8Uo39.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt # recall that the magnitude/length of a vector [g[0], g[1], g[2]] is sqrt(g[0]^2 + g[1]^2 + g[2]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix= feature_matrix, weights= weights)\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]:\n",
    "            derivative = feature_derivative(errors,feature_matrix.values[:,i])\n",
    "            # add the squared value of the derivative to the gradient sum of squares (for assessing convergence)\n",
    "#             print(derivative)\n",
    "            gradient_sum_squares += derivative ** 2\n",
    "\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            \n",
    "            weights[i] = weights[i] - derivative * step_size\n",
    "        # compute the square-root of the gradient sum of squares to get the gradient magnitude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "#         print(gradient_magnitude)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "用第一次的實驗的資料 \n",
    "(\n",
    "Intercept: -47116.0790729\n",
    "Slope: 281.95883963\n",
    ") \n",
    "\n",
    "來試試看 \n",
    "\n",
    "可以看到很接近但不完全一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-46999.88716555,    281.91211918])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = regression_gradient_descent(train_data[[\"constant\",\"sqft_living\"]], train_data['price'], initial_weights, step_size, tolerance)\n",
    "weight #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356134.44325500238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = predict_output(weights=weight,feature_matrix=test_data[[\"constant\",\"sqft_living\"]])\n",
    "test_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.99999688e+04,   2.45072603e+02,   6.52795267e+01])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9\n",
    "model2_weight = regression_gradient_descent(train_data[[\"constant\",'sqft_living', 'sqft_living15']], train_data['price'], initial_weights, step_size, tolerance)\n",
    "model2_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366651.41162949387"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = predict_output(weights=model2_weight,feature_matrix=test_data[[\"constant\",'sqft_living', 'sqft_living15']])\n",
    "test_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"price\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mylab]",
   "language": "python",
   "name": "conda-env-mylab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
